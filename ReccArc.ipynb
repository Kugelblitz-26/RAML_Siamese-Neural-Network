{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g60lZmv2lfb8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1. SETUP AND IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "# Import our updated project modules\n",
    "from data.processing import load_and_prepare_data, create_book_pairs, get_metadata_features, get_text_features\n",
    "from model.encoders import MLPEncoder, BERTEncoder # CNN and Hybrid can be added later\n",
    "from model.siamese import SiameseModel\n",
    "from train.losses import contrastive_loss\n",
    "from evaluate.plots import plot_roc_curves\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 1\n",
    "EMBEDDING_DIM = 128\n",
    "MAX_TITLE_LENGTH = 25 # Increased slightly for book titles\n",
    "DATA_PATH = \"data/kaggle_data/\"\n",
    "# Use a sample size for faster experiments!\n",
    "PAIR_SAMPLE_SIZE = 20000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 388
    },
    "id": "3xFuYPfKoq5i",
    "outputId": "de221c91-4052-4cd1-c2a2-5df40b14b3b7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## 2. Data Loading and Pair Creation\n",
    "\n",
    "\n",
    "# Load and process the new dataset\n",
    "reviews_df, books_df = load_and_prepare_data(data_path=DATA_PATH)\n",
    "\n",
    "# Create book pairs using the reviews data\n",
    "pairs_df = create_book_pairs(reviews_df, min_ratings=5, sample_size=PAIR_SAMPLE_SIZE)\n",
    "\n",
    "# Filter out pairs where one of the books might not have metadata\n",
    "valid_book_ids = set(books_df['book_id'])\n",
    "pairs_df = pairs_df[pairs_df['book_a'].isin(valid_book_ids) & pairs_df['book_b'].isin(valid_book_ids)]\n",
    "\n",
    "print(f\"Total pairs for training/testing: {len(pairs_df)}\")\n",
    "display(pairs_df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hACelGLEstuf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568,
     "referenced_widgets": [
      "46ba470a0d9d49e9a49259668533605d",
      "7c073a1d2b874c99b5a9cc2f36dea7b8",
      "8a2691cb06334c869d17d42986897dba",
      "ced69cd1619847669e9f33dafd76c89d",
      "9fb781f7e67e40b287652dcee6a83e8a",
      "01b51f0e25a1423a9f8da351baa38e3d",
      "b045088fc9d2431990eddcc43c3ac2ed",
      "e1fa50c9d8214fca91437a614aebec80",
      "34cf11087ea04f61915ed05b67e8b3c6",
      "f90f72a8d6c546bf8a897ecafa1e62db",
      "7840cb7654284b0abe9ccdca27e7f672",
      "6298d5c38c4f41a5a77705de28885425",
      "fe11bd6f8f864e079a8334c7f854d892",
      "74683f1f1b344320963673f935e2c1c8",
      "dcb8c6b389e34916bae10cbb61ee3a1b",
      "1c562cc407ad4114a493b2c500ee1b7d",
      "90b462439e864e5fba962b9bc91264e0",
      "bbe682d6b6cd488ebd1d4a878af13410",
      "133290a6374c47638a60b8259a36905e",
      "31534fd30e49423f83781d875e7d0884",
      "cddec38807754795a9f3dddd87bd641d",
      "3c1d05184c1e4f0a90aeeba6c912c6d1",
      "828dbcd3a8154d369fa1934da77f91b2",
      "183af9a4bc9d4dcd938d79982f3e5006",
      "4d06c3abb8c94cd2bc2a58bea08653fc",
      "7ce98e3ceaa04f0bb4dffb0e057b9d6f",
      "fdbfb552316041aaa6c3004c158ad402",
      "0f992deb9c1b40d7b8ae229e446819d0",
      "ee14cdea97614e49976541d4f202b351",
      "ab8c4bf1eece4f90b0ae5211384ec32d",
      "6e34f92eecf347d0b1c10c5499b3120b",
      "0c4a239e7b12417abaded39fc7b1c4b5",
      "f94a05a5b54f406abd566fd9e4e5147f",
      "0081b8bf96fb498eaada04336cddd6d6",
      "6eb42da8233c40d6bae2c6ab2741e120",
      "81f78e1dd3bc4797a5e3ee200002dbed",
      "8931fbb330704f5a8bde0fea01898a81",
      "b4db5f779f8d47f0af977ce605983a5a",
      "8ae9a1e3d237478b9f4789ded539586b",
      "65f1b2d29c73403583af51187e1d0d96",
      "b25918a092ab48e281ffb6512355c238",
      "2ff67e83cfb54634bdd8a119ec78825e",
      "6b67eae5ede3480d89436ac4dc999945",
      "f57bf90a16284c8b95418d0a15e6ab75"
     ]
    },
    "id": "sxCkS2NporFW",
    "outputId": "26f7e00c-feb3-4149-c28a-2cafd19a401d"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## 3. Feature Engineering\n",
    "\n",
    "# A. Metadata Features for MLP\n",
    "print(\"Preparing metadata features...\")\n",
    "metadata_features = get_metadata_features(books_df)\n",
    "print(\"Metadata features shape:\", metadata_features.shape)\n",
    "display(metadata_features.head())\n",
    "\n",
    "\n",
    "# B. Text Features for CNN/BERT\n",
    "print(\"\\nPreparing text features...\")\n",
    "text_features = get_text_features(books_df)\n",
    "tokenizer_bert = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "def tokenize_for_bert(titles):\n",
    "    return tokenizer_bert(\n",
    "        titles,\n",
    "        max_length=MAX_TITLE_LENGTH,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_tensors='tf'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0C4MxweZorM8",
    "outputId": "9c619750-131a-4055-bd73-3b86382fbb81"
   },
   "outputs": [],
   "source": [
    "\n",
    "# ## 4. Preparing Data for TensorFlow (New, High-Performance Method)\n",
    "#\n",
    "# Instead of a slow Python generator, we will prepare all data in memory as NumPy arrays.\n",
    "# This is much more efficient and the standard way to do it for datasets that fit in memory.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(pairs_df, test_size=0.2, random_state=42)\n",
    "\n",
    "def prepare_data_for_model(df, metadata_map, text_map):\n",
    "    \"\"\"\n",
    "    Pre-generates all data into NumPy arrays for maximum performance.\n",
    "    \"\"\"\n",
    "    # Metadata for MLP\n",
    "    book_a_meta = np.array([metadata_map.loc[id].values for id in df['book_a']])\n",
    "    book_b_meta = np.array([metadata_map.loc[id].values for id in df['book_b']])\n",
    "\n",
    "    # Text for BERT/CNN (Tokenization)\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    titles_a = text_map.loc[df['book_a']]['title'].tolist()\n",
    "    titles_b = text_map.loc[df['book_b']]['title'].tolist()\n",
    "\n",
    "    tokens_a = tokenizer(titles_a, max_length=MAX_TITLE_LENGTH, truncation=True, padding='max_length', return_tensors='np')\n",
    "    tokens_b = tokenizer(titles_b, max_length=MAX_TITLE_LENGTH, truncation=True, padding='max_length', return_tensors='np')\n",
    "\n",
    "    # Labels\n",
    "    labels = df['label'].values\n",
    "\n",
    "    return {\n",
    "        \"mlp_input\": [book_a_meta, book_b_meta],\n",
    "        \"bert_input\": [\n",
    "            {'input_ids': tokens_a['input_ids'], 'attention_mask': tokens_a['attention_mask']},\n",
    "            {'input_ids': tokens_b['input_ids'], 'attention_mask': tokens_b['attention_mask']}\n",
    "        ],\n",
    "        \"labels\": labels\n",
    "    }\n",
    "\n",
    "print(\"Preparing training data into NumPy arrays...\")\n",
    "train_data = prepare_data_for_model(train_df, metadata_features, text_features)\n",
    "\n",
    "print(\"Preparing validation data into NumPy arrays...\")\n",
    "val_data = prepare_data_for_model(test_df, metadata_features, text_features)\n",
    "\n",
    "experiments = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215,
     "referenced_widgets": [
      "16d276d04f3644d3837ba45deeb71b2f",
      "0f646e373a224eeda1fba330ed40f90d",
      "bd7e8fa698224752b3b8b6ee53b9fa61",
      "5c52237bf8ff4b4ba5e0582b5831dc39",
      "6be9da78bdb5487397cd7c5b9ae17ede",
      "1aaf46aec7af468da37affba7ff47fe8",
      "7aac87bb29724aac99a37df49bf498b6",
      "417bb53f281844bf9c399e725b14abe1",
      "13e7245338cd496bbc379fd7cac21e88",
      "1671820b6d8c485188f863380488d6a6",
      "1ad0f15f35144b8cac8f64d2a640478e"
     ]
    },
    "id": "GFPv75BforQh",
    "outputId": "0552eaaa-3997-4dd1-9816-348adba72032"
   },
   "outputs": [],
   "source": [
    "# --- file: notebooks/experiments.ipynb ---\n",
    "\n",
    "# --- Experiment 2: BERT (Using a Subclassed Encoder) ---\n",
    "\n",
    "print(\"\\n--- Starting BERT Experiment ---\")\n",
    "\n",
    "# 1. Define the model by creating an INSTANCE of our new class.\n",
    "#    No max_length is needed here as the model is not symbolic.\n",
    "bert_encoder = BERTEncoder(embedding_dim=EMBEDDING_DIM)\n",
    "bert_siamese = SiameseModel(bert_encoder)\n",
    "bert_siamese.compile(optimizer=Adam(5e-5), loss=contrastive_loss) # Lower LR for fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_ETsiVZyMaf"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BGcE0GFcorUA",
    "outputId": "b6db603e-313f-496f-f2ab-a94dc3f23daa"
   },
   "outputs": [],
   "source": [
    "# --- file: notebooks/experiments.ipynb ---\n",
    "\n",
    "# --- Experiment 2: BERT (Using a Subclassed Encoder) ---\n",
    "\n",
    "print(\"\\n--- Starting BERT Experiment ---\")\n",
    "\n",
    "# 1. Define the model by creating an INSTANCE of our new class.\n",
    "#    No max_length is needed here as the model is not symbolic.\n",
    "bert_encoder = BERTEncoder(embedding_dim=EMBEDDING_DIM)\n",
    "bert_siamese = SiameseModel(bert_encoder)\n",
    "bert_siamese.compile(optimizer=Adam(5e-5), loss=contrastive_loss) # Lower LR for fine-tuning\n",
    "# 2. The data pipeline is already correct and doesn't need to be changed.\n",
    "train_ds_bert = tf.data.Dataset.from_tensor_slices((\n",
    "    (train_data['bert_input'][0], train_data['bert_input'][1]),\n",
    "    train_data['labels']\n",
    ")).shuffle(1024).batch(BATCH_SIZE).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds_bert = tf.data.Dataset.from_tensor_slices((\n",
    "    (val_data['bert_input'][0], val_data['bert_input'][1]),\n",
    "    val_data['labels']\n",
    ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# 3. Calculate steps for model.fit()\n",
    "steps_per_epoch_bert = len(train_df) // BATCH_SIZE\n",
    "validation_steps_bert = len(test_df) // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRVrWoJGxNr7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 4. Fit the model\n",
    "print(\"Starting BERT model training...\")\n",
    "start_time = time.time()\n",
    "history_bert = bert_siamese.fit(\n",
    "    train_ds_bert,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch_bert,\n",
    "    validation_data=val_ds_bert,\n",
    "    validation_steps=validation_steps_bert\n",
    ")\n",
    "bert_time = time.time() - start_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78-H9Cu6p8hc"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 5. Evaluation\n",
    "print(\"Evaluating BERT model...\")\n",
    "y_pred_bert = bert_siamese.predict(val_ds_bert, steps=validation_steps_bert)\n",
    "y_true_bert = val_data['labels'][:len(y_pred_bert)]\n",
    "\n",
    "experiments['BERT'] = {\n",
    "    'model': bert_siamese,\n",
    "    'y_pred': y_pred_bert,\n",
    "    'y_true': y_true_bert,\n",
    "    'time': bert_time,\n",
    "    'auc': roc_auc_score(y_true_bert, 1 - y_pred_bert)\n",
    "}\n",
    "print(f\"BERT AUC: {experiments['BERT']['auc']:.4f}, Time: {bert_time:.2f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# ## 5. Training and Evaluation Loop\n",
    " \n",
    "EPOCHS = 3\n",
    "\n",
    "\n",
    "# --- Experiment 1: MLP ---\n",
    "print(\"\\n--- Starting MLP Experiment ---\")\n",
    "\n",
    "# Create the optimized tf.data.Dataset\n",
    "train_ds_mlp = tf.data.Dataset.from_tensor_slices((\n",
    "    (train_data['mlp_input'][0], train_data['mlp_input'][1]), \n",
    "    train_data['labels']\n",
    ")).shuffle(1024).batch(BATCH_SIZE).repeat().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "val_ds_mlp = tf.data.Dataset.from_tensor_slices((\n",
    "    (val_data['mlp_input'][0], val_data['mlp_input'][1]),\n",
    "    val_data['labels']\n",
    ")).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "# Define the model\n",
    "mlp_input_shape = (metadata_features.shape[1],)\n",
    "mlp_encoder = MLPEncoder(input_shape=mlp_input_shape, embedding_dim=EMBEDDING_DIM)\n",
    "mlp_siamese = SiameseModel(mlp_encoder)\n",
    "mlp_siamese.compile(optimizer=Adam(0.001), loss=contrastive_loss)\n",
    "\n",
    "# Calculate steps_per_epoch\n",
    "steps_per_epoch = len(train_df) // BATCH_SIZE\n",
    "validation_steps = len(test_df) // BATCH_SIZE\n",
    "\n",
    "start_time = time.time()\n",
    "history_mlp = mlp_siamese.fit(\n",
    "    train_ds_mlp, \n",
    "    epochs=EPOCHS, \n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=val_ds_mlp,\n",
    "    validation_steps=validation_steps\n",
    ")\n",
    "mlp_time = time.time() - start_time\n",
    "\n",
    "# --- Evaluation ---\n",
    "y_pred_mlp = mlp_siamese.predict(val_ds_mlp, steps=validation_steps)\n",
    "y_true_mlp = val_data['labels'][:len(y_pred_mlp)] # Ensure labels match prediction size\n",
    "\n",
    "experiments['MLP'] = {\n",
    "    'model': mlp_siamese,\n",
    "    'y_pred': y_pred_mlp,\n",
    "    'y_true': y_true_mlp,\n",
    "    'time': mlp_time,\n",
    "    'auc': roc_auc_score(y_true_mlp, 1 - y_pred_mlp)\n",
    "}\n",
    "print(f\"MLP AUC: {experiments['MLP']['auc']:.4f}, Time: {mlp_time:.2f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Quantitative Comparison\n",
    "summary_data = []\n",
    "for name, result in experiments.items():\n",
    "    summary_data.append({\n",
    "        'Architecture': name,\n",
    "        'ROC-AUC': result['auc'],\n",
    "        'Training Time (s)': result['time']\n",
    "    })\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"--- Final Results Summary ---\")\n",
    "display(summary_df)\n",
    "\n",
    "\n",
    "# --- Visual Comparison: ROC Curves ---\n",
    "print(\"\\n--- Generating ROC Curves ---\")\n",
    "plot_roc_curves(experiments)\n",
    "\n",
    "\n",
    "# --- Visual Comparison: Embeddings (example with BERT) ---\n",
    "print(\"\\n--- Generating Embedding Visualization for BERT Encoder ---\")\n",
    "# 1. Create a sample of 1000 book IDs to visualize\n",
    "sample_book_ids = np.random.choice(text_features.index, 1000, replace=False)\n",
    "sample_titles = text_features.loc[sample_book_ids]['title'].tolist()\n",
    "sample_labels = np.random.randint(0, 10, size=1000) # Placeholder labels\n",
    "\n",
    "# 2. Tokenize the sampled titles for the BERT encoder\n",
    "tokenized_samples = tokenize_for_bert(sample_titles) # Assumes tokenize_for_bert is defined\n",
    "bert_input_data = {'input_ids': tokenized_samples['input_ids'], 'attention_mask': tokenized_samples['attention_mask']}\n",
    "\n",
    "\n",
    "# 3. Call the new, simpler plotting function\n",
    "#    It now correctly uses the data we prepared right here.\n",
    "plot_embeddings(bert_encoder, data=bert_input_data, labels=sample_labels)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
